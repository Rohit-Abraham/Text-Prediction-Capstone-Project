---
title: 'Capstone Project: Milestone Report'
author: "Rohit Benny Abraham"
date: "8/25/2020"
output:
  word_document: default
  pdf_document: default
  html_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE)
knitr::opts_chunk$set(cache = TRUE)
knitr::opts_knit$set(verbose = TRUE)
```

## Overview

Lacking a full-size keyboard on our mobile device has made text entry on touch
screen devices a tedious job. Automated text prediction aims to solve this by 
using entered text to predict the next word.

This report does an exploratory analysis of a large body of text to design a 
text prediction system that could be run efficiently on a mobile device.

RStudio and the R package tm for text mining were used to perform the analysis.
The final prediction model will consist of a transition matrix (which 
represents a Markov process) that can be easily imported into a Shiny app.

## Data Acquistion

Predictive text modeling using NLP follows generally the same approach to data 
as any other prediction problem. The data is obtained, cleaned and explored, 
before moving to the predictive modeling stage using a training, validation and
test set, and finally text prediction itself.

The Coursera dataset analyzed for this report is downloaded from this 
![link]([https://d396qusza40orc.cloudfront.net/dsscapstone/dataset/Coursera-SwiftKey.zip])
There seem to be three different categories of text (“blogs”, “news” and 
“twitter”) for four different languages (German, English, finnish and Russian).
For this analysis I used only the text files in American english language.

## Data Preprocessing

### Loading dependencies

```{r dependencies}
options(java.parameters = "- Xmx1024m")

if(!("tm" %in% rownames(installed.packages()))) {
    install.packages("tm")
}
suppressMessages(library(tm))

if(!("RWeka" %in% rownames(installed.packages()))) {
    install.packages("RWeka")
}
suppressMessages(library(RWeka))

if(!("ggplot2" %in% rownames(installed.packages()))) {
    install.packages("ggplot2")
}
suppressMessages(library(ggplot2))

if(!("wordcloud" %in% rownames(installed.packages()))) {
    install.packages("wordcloud")
}
suppressMessages(library(wordcloud))
```


### Importing the data

Basic outline of the corpus used:

```{r import}
# Importing the corpus
dir <- "D:/Coursera/Capstone Project/final/en_US"
corp <- Corpus(DirSource(dir), readerControl = list(reader = readPlain,
                                                    language = "en_US", 
                                                    load = TRUE))
summary(corp)
```

### Calculating the memory attributes of the files in corpus

Summary of the files in corpus:

```{r memory}
objSize <- format(object.size(corp), units = "Mb")

countLines <- 0
for (i in 1:length(corp))
        countLines[i] <- c(stringr::str_count(string = corp[[i]], 
                                              pattern = "\\n"))

# Get file size in Mb
getSize <- function(file){
        size <- file.info(file)$size/(1024*1024)
        return(size)
}

dirFiles<-c(paste0(dir,"/en_US.blogs.txt"),paste0(dir,"/en_US.news.txt"),
            paste0(dir,"/en_US.twitter.txt"))
fileSize<-sapply(dirFiles,getSize)

# Number of words in each file
getWords <- function(charVec){
        wordsCount <- sum(sapply(gregexpr("\\S+", charVec), length))
        return(wordsCount)
}

wordsCount <- sapply(as.list(corp), getWords)

# Corpus summary
corpSummary <- data.frame("File Size(Mb)" = fileSize,
                          "No. of Lines" = countLines,
                          "No. of words" = wordsCount)
corpSummary[4, ] <- colSums(corpSummary)
row.names(corpSummary) <- c("blogs", "news", "twitter", "total")
knitr::kable(corpSummary)
```

- It can be seen that when loading all 3 text files into corpus, the object
occupies 375.7 Mb memory and in total we got 3.336692^{6} lines of text to
handle.

- To reduce the computational cost we will go with sampling i.e. ranodmly 
selecting the lines needed to be inlcuded to get an accurate assumaption to 
results that would be obtained using all the data.

### Sampling

Created a separate sub-sample dataset by reading in a random subset of the
original data and writing it out to a separate file (here I use rbinom function
to “flip a biased coin” to decide which line to read in with a sampling rate of
10%)

```{r sampling}
# Reading individual file and sampling
blogs <- readLines(paste(getwd(),"/en_US/en_US.blogs.txt", sep = ""))
set.seed(123)
blogs <- blogs[rbinom(length(blogs)*0.1, length(blogs), 0.5)]
blogs <- iconv(blogs,"latin1","ASCII",sub="")
write.table(blogs, file = paste(getwd(), "/sample/blogsSample.csv", sep = ""), 
                              row.names = FALSE, col.names = FALSE, sep = ',')

news <- readLines(paste(getwd(),"/en_US/en_US.news.txt", sep = ""))
set.seed(123)
news <- news[rbinom(length(news)*0.1, length(news), 0.5)]
news <- iconv(news,"latin1","ASCII",sub="")
write.table(news, file = paste(getwd(), "/sample/newsSample.csv", sep = ""), 
          row.names = FALSE, col.names = FALSE, sep = ',')

twitter <- readLines(paste(getwd(),"/en_US/en_US.twitter.txt", sep = ""),
                     encoding = 'UTF-16', skipNul = TRUE)
set.seed(123)
twitter <- twitter[rbinom(length(twitter)*0.1, length(twitter), 0.5)]
twitter <- iconv(twitter,"latin1","ASCII",sub="")
write.table(twitter, file = paste(getwd(), "/sample/twitterSample.csv", 
                                  sep = ""), 
          row.names = FALSE, col.names = FALSE, sep = ',')
```

## Text  Mining

### Creating a corpus of the sample data

```{r sample corpus, echo=TRUE}
corpus <- VCorpus(DirSource(paste(getwd(), "/sample", sep = "")),
                     readerControl = list(reader = readPlain,
                                          language = "en_US",
                                          load = TRUE))
```

### Corpus cleaning

Stemming is not performed in our analysis and the stop words are treated for now
to visualize and analyze the gram models, so that the frequency of the stop 
words does not skew the observations. In general, if we were working with 
feature extraction (for machine learning), we would do that, but given that stop
words are relevant for predicting text typing in natural languages, we will keep
them in our modeling.

```{r preprocess, echo=TRUE}
replacePunctuation <- content_transformer(function(x) {
        gsub("[^[:alnum:][:space:]'`]", " ", x)
})

clean_corpus <- function(corpus) {
        corpus <- tm_map(corpus, stripWhitespace)
        corpus <- tm_map(corpus, replacePunctuation)
        corpus <- tm_map(corpus, removeNumbers)
        corpus <- tm_map(corpus, removeWords, stopwords("en"))
        corpus <- tm_map(corpus, content_transformer(tolower))
        corpus
}

corpus <- clean_corpus(corpus)

profanityList <- read.csv("profanity_words.txt", header = FALSE,
                          stringsAsFactors = FALSE)
profanityWords <- profanityList$V1
corpus <- tm_map(corpus, removeWords, profanityWords)
```

## Exploratory data analysis

### Creating and visulaizing uni-grams

```{r unigrams}
tdm <- TermDocumentMatrix(corpus)
gramcorpus <- findFreqTerms(tdm)
count <- rowSums(as.matrix(tdm[gramcorpus, ]))
gramdata <- data.frame(word = names(count), frequency = count)
gramdatasorted <- gramdata[order(-gramdata$frequency), ]

ggplot(gramdatasorted[1:15, ], aes(x = reorder(word, -frequency), 
                                   y = frequency)) +
        geom_bar(stat = 'identity') +
        xlab("Words in corpus") + ylab("Count") + ggtitle("Top 15 one-grams")
```

```{r wordcloud1}
words_cloud <- function(ngrams){
        wordcloud(ngrams$word, ngrams$frequency, scale = c(4, 0.5), 
                  max.words = 100, min.freq = 5, 
                  colors = brewer.pal(8, "Dark2"))
}

suppressWarnings(words_cloud(gramdatasorted))
```

### Creating and visulaizing bi-grams

```{r bigrams}
biGram <- function(x){
        NGramTokenizer(x, Weka_control(min = 2, max = 2))
}

bigramtdm <- TermDocumentMatrix(corpus, control = list(tokenize = biGram))
bigramcorpus <- findFreqTerms(bigramtdm)
bigramcount <- rowSums(as.matrix(bigramtdm[bigramcorpus, ]))
bigramdata <- data.frame(word = names(bigramcount), frequency = bigramcount)
bigramdatasorted <- bigramdata[order(-bigramdata$frequency), ]

ggplot(bigramdatasorted[1:15, ], aes(x = reorder(word, -frequency), 
                                   y = frequency)) +
        geom_bar(stat = 'identity') +
        xlab("Words in corpus") + ylab("Count") + ggtitle("Top 15 bigrams")
```


```{r wordcloud2}
suppressWarnings(words_cloud(bigramdatasorted))
```

## Creating and visulaizing tri-grams

```{r trigrams}
triGram <- function(x){
        NGramTokenizer(x, Weka_control(min = 3, max = 3))
}

trigramtdm <- TermDocumentMatrix(corpus, control = list(tokenize = triGram))
trigramcorpus <- findFreqTerms(trigramtdm)
trigramcount <- rowSums(as.matrix(trigramtdm[trigramcorpus, ]))
trigramdata <- data.frame(word = names(trigramcount), frequency = trigramcount)
trigramdatasorted <- trigramdata[order(-trigramdata$frequency), ]

ggplot(trigramdatasorted[1:15, ], aes(x = reorder(word, -frequency), 
                                     y = frequency)) +
        geom_bar(stat = 'identity') +
        xlab("Words in corpus") + ylab("Count") + ggtitle("Top 15 trigrams")
```

```{r wordcloud3}
suppressWarnings(words_cloud(trigramdatasorted))
```

## Creating 4-grams

Computed 4-grams for the as we will be needing this size of n-grams for our 
prediction model

```{r fourgrams}
fourGram <- function(x){
        NGramTokenizer(x, Weka_control(min = 4, max = 4))
}

fourgramtdm <- TermDocumentMatrix(corpus, control = list(tokenize = fourGram))
fourgramcorpus <- findFreqTerms(fourgramtdm)
fourgramcount <- rowSums(as.matrix(fourgramtdm[fourgramcorpus, ]))
fourgramdata <- data.frame(word = names(fourgramcount), frequency = fourgramcount)
fourgramdatasorted <- fourgramdata[order(-fourgramdata$frequency), ]
fourgramdatasorted[1:5, ]
```

### Word coverage

Word coverage is determined by the count of most frequent words required to 
cover a given percentage of words present in the entire data-set.

We are going to answer this question by finding out how the number of words 
covered in the data-set increases as we add words from the most frequent to the 
least frequent.

```{r coverage, echo=TRUE}
wordsCoverage <- data.frame(
        coverage = round(cumsum(gramdatasorted$frequency)/sum(gramdatasorted$frequency)*100, 2),
        words = 1:nrow(gramdatasorted))

wordsCoverage[1:5, ]
```

Now we need to know what’s the minimum number of top words added to achieve 50% and 90% coverage:

```{r half coverage, echo=TRUE}
halfCoverage <- min(wordsCoverage[wordsCoverage$coverage > 50, ]$words)
halfCoverage
```

```{r ninety coverage, echo=TRUE}
ninetyCoverage <- min(wordsCoverage[wordsCoverage$coverage > 90, ]$words)
ninetyCoverage
```

According with these computations, we would need 740 words to achieve 50% 
coverage (i.e. the minimum number of frequent words added that would represent 
half of the words present in the data) and 7659 words to achieve 90% coverage.

Plot the coverage 

```{r coverage plot}
ggplot(wordsCoverage, aes(x = words, y = coverage)) + 
        geom_area(color = "darkblue", fill = "lightblue", size = 1, 
                  alpha = 0.5) + 
        ggtitle("Word coverage for the corpus") +
        xlab("Most frequent words") +
        ylab("Coverage percentage")
```

## Future work

The future work will focus on building some ngram-based statistical language 
models trained on a much larger sample of the original dataset, prune these 
models with a validation set and choose the best model based on a testset, with
sentences never used in training or validation. When building these models, 
we’ll try to optimize storage and memory utilization keeping in mind the
constraints of free Shiny App platform.

For unseen ngrams, some type of smoothing(backoff) will be used to deal with the 
zero probabilities.

However, this approach is computationally very expensive - the required matrices
get very large and it’s very likely that for a decent-sized training set my 
available computer will get overwhelmed. Therefore there is need to explore 
alternative approaches.